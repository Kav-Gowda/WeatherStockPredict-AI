# -*- coding: utf-8 -*-
"""WeatherStockPredict-AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TGYkY3hbO4hDTLNm8Slfaa2uAe_-oAyr
"""

# --- Install required libraries (Colab-friendly) ---
!pip install -q tqdm seaborn scikit-learn==0.24 statsmodels

# --- Imports & global settings ---
from functools import reduce
from copy import deepcopy
import numpy as np
import pandas as pd
import tqdm
from scipy.signal import periodogram
from scipy.stats import binomtest
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller, kpss
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
import seaborn as sns
import warnings

# Pandas/plot styling
pd.set_option('display.float_format', lambda x: '%.8f' % x)
warnings.filterwarnings('ignore')
sns.set_context('notebook')
sns.set(style="darkgrid")
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 5))
plt.rc("axes", labelweight="bold", labelsize="large", titleweight="bold", titlesize=16, titlepad=10)

# --- Import datasets (local files; Colab upload fallback) ---
# Expect two files in your working directory:
#   - laguardia.csv  (weather observations)
#   - dow_jones.csv  (DJI OHLCV daily)
import os

def ensure_files(files):
    missing = [f for f in files if not os.path.exists(f)]
    if missing:
        try:
            from google.colab import files
            print("Please upload the following files:", missing)
            files.upload()  # Select the required CSVs from your machine
        except Exception:
            raise FileNotFoundError(f"Missing files: {missing}. Place them in the working directory.")

ensure_files(["laguardia.csv", "dow_jones.csv"])

# Read as strings first (safer), convert later
laguardia = pd.read_csv('laguardia.csv', dtype='str')
dow = pd.read_csv('dow_jones.csv', dtype='str')

print("laguardia shape:", laguardia.shape, "| dow shape:", dow.shape)

# --- Convert columns to correct types & select columns of interest ---
# Weather data
laguardia['DATE'] = pd.to_datetime(laguardia['DATE'])
laguardia[['wind','dew_point','temp','pressure','cloud_cover']] = (
    laguardia[['wind','dew_point','temp','pressure','cloud_cover']].astype(float)
)

# Market data
dow['DATE'] = pd.to_datetime(dow['DATE'])
# Drop 'na' rows in strings, then convert numerics
dow = dow.loc[dow['Open'].str.strip().ne('na')]
num_cols = [c for c in dow.columns if c != 'DATE']
dow[num_cols] = dow[num_cols].astype(float)
if 'Volume' in dow.columns:
    # Sometimes volume is float after above cast; ensure int if possible
    try:
        dow['Volume'] = dow['Volume'].astype(int)
    except Exception:
        pass

# Keep only required columns
laguardia = laguardia.loc[:, ['DATE', 'temp', 'cloud_cover']]
dow = dow.loc[:, ['DATE', 'Close']]

print(laguardia.dtypes, "\n")
print(dow.dtypes)

# --- EDA: time resolution & basic checks ---
print("laguardia 'DATE' field head")
print(laguardia.DATE.head())

print("\nlaguardia 'DATE' field hour availability")
print(sorted(laguardia.DATE.dt.hour.unique()))

print("\nlaguardia 'DATE' field minute availability")
print(sorted(laguardia.DATE.dt.minute.unique()))

print("\ndow 'DATE' field head")
print(dow.DATE.head())

print("\ndow 'DATE' field hour availability")
print(sorted(dow.DATE.dt.hour.unique()))

print("\ndow 'DATE' field minute availability")
print(sorted(dow.DATE.dt.minute.unique()))

print("\nlaguardia 'DATE' field minute frequency (head):")
print(laguardia.DATE.dt.minute.value_counts().head())

print("\n'laguardia' duplicated:")
print(laguardia.DATE.duplicated().value_counts())

print("\n'dow' duplicated:")
print(dow.DATE.duplicated().value_counts())

print("\n'laguardia' missing:")
print(laguardia.isna().max())

print("\n'dow' missing:")
print(dow.isna().max())

pd.set_option('display.float_format', lambda x: '%.2f' % x)
print("\n'laguardia' description:")
print(laguardia.describe())
print("\n'dow' description:")
print(dow.describe())
pd.set_option('display.float_format', lambda x: '%.8f' % x)

# --- Step 1: Resample weather data to hourly by rounding to the hour, then averaging by column ---
laguardia_rounded = laguardia.copy()
laguardia_rounded['DATE'] = laguardia_rounded['DATE'].dt.round('60min')

laguardia_cols = []
for c in ['temp', 'cloud_cover']:
    col_df = laguardia_rounded[['DATE', c]].dropna().groupby('DATE', as_index=False).agg({c: 'mean'})
    laguardia_cols.append(col_df)

laguardia_merged = reduce(lambda l, r: pd.merge(l, r, on='DATE', how='outer'), laguardia_cols)
laguardia_merged.sort_values('DATE', inplace=True)
laguardia_merged.head()

# --- Step 2: Handle missing hours & interpolate linearly ---
print(laguardia_merged.DATE.diff().value_counts().head())

# Reindex to strict hourly index, then interpolate
laguardia_merged.set_index('DATE', drop=True, inplace=True)
laguardia_merged = laguardia_merged.reindex(
    pd.date_range(start=laguardia_merged.index.min(), end=laguardia_merged.index.max(), freq='1H')
).astype(float)

laguardia_merged.interpolate(method='linear', inplace=True)

print("\nAfter interpolation, any missing?")
print(laguardia_merged.isna().value_counts())
laguardia_merged.describe()

# --- Step 3: Average weather between 8:00 and 9:00 each day (pre-market) ---
laguardia_merged_avg = laguardia_merged.between_time('8:00', '9:00').reset_index()
laguardia_merged_avg.rename({'index': 'DATE'}, axis=1, inplace=True)
laguardia_merged_avg['DATE'] = laguardia_merged_avg['DATE'].dt.round('1D')
laguardia_merged_avg = (
    laguardia_merged_avg.groupby('DATE', as_index=False)
    .agg({'temp': 'mean', 'cloud_cover': 'mean'})
    .set_index('DATE')
)

laguardia_merged_avg.rename(
    columns={c: f"{c}_avg" for c in laguardia_merged_avg.columns},
    inplace=True
)

df_weather_final = laguardia_merged_avg
df_weather_final.head()

# --- Step 4: Merge daily weather with DJI (keep all weather rows; we'll subset later) ---
dow.sort_values('DATE', inplace=True)
df = dow.merge(
    df_weather_final, how='outer', left_on='DATE', right_index=True
).set_index('DATE').sort_index()

df = df.loc[df.index >= df_weather_final.index[0]]
df.head()

# --- Construct outcome variable: log price & log differences (returns) ---
_ = sns.lineplot(data=df['Close']).set_title('DJI Close Price')

df['log_Close'] = np.log(df['Close'])
_ = sns.lineplot(data=df['log_Close']).set_title('Log DJI Close Price')

# Proper differencing on available log_Close values only
log_Close = deepcopy(df['log_Close'])
log_Close.dropna(inplace=True)
ld_Close = log_Close.diff()

df = df.merge(
    pd.DataFrame(ld_Close).rename({'log_Close':'ld_Close'}, axis=1),
    how='left', left_index=True, right_index=True
)

_ = sns.lineplot(data=df['ld_Close']).set_title('Log differenced DJI Close Price')

print('p-value of ADF test:')
print(adfuller(df.ld_Close.dropna())[1])
print('p-value of KPSS test:')
print(kpss(df.ld_Close.dropna())[1])

# --- Periodogram helper & plots (stationarity / seasonality diagnostics) ---
def plot_periodogram(ts, detrend='linear', ax=None):
    fs = pd.Timedelta("365D6H") / pd.Timedelta("1D")
    freqencies, spectrum = periodogram(
        ts, fs=fs, detrend=detrend, window="boxcar", scaling='spectrum'
    )
    if ax is None:
        _, ax = plt.subplots()
    ax.step(freqencies, spectrum, color="purple")
    ax.set_xscale("log")
    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 73, 104])
    ax.set_xticklabels(
        ["Annual (1)", "Semiannual (2)", "Quarterly (4)", "Bimonthly (6)",
         "Monthly (12)", "Biweekly (26)", "Weekly (52)", "5-day Week (73)",
         "Semiweekly (104)"],
        rotation=30,
    )
    ax.ticklabel_format(axis="y", style="sci", scilimits=(0, 0))
    ax.set_ylabel("Variance")
    ax.set_title("Periodogram")
    return ax

plot_periodogram(df.loc[:, 'ld_Close'].dropna())
plot_periodogram(df.loc[:, 'log_Close'].dropna())

# --- Make features stationary: Seasonal adjustment of temperature, then difference if needed ---
_ = sns.lineplot(data=df['temp_avg']).set_title('Average temperature between 8 and 9 am')

plot_periodogram(df.loc[:, 'temp_avg'].dropna())

# Fit polynomial seasonal component on first 16 years (up to 1964-01-01 inclusive)
y = df.loc[df.index < '1964-01-02', 'temp_avg']
X = [i % 365.25 for i in range(0, len(y.to_numpy()))]
X_full = [i % 365.25 for i in range(0, len(df.temp_avg.dropna().to_numpy()))]
degree = 4
coef = np.polyfit(X, y.to_numpy(), degree)
print('Coefficients:', coef)

temp_sc_avg = []
for i in range(len(X_full)):
    value = coef[-1]
    for d in range(degree):
        value += X_full[i]**(degree-d) * coef[d]
    temp_sc_avg.append(value)

# Align seasonal component length with df.temp_avg index (dropna alignment)
temp_index = df['temp_avg'].dropna().index
temp_sc_series = pd.Series(temp_sc_avg, index=temp_index)
df['temp_sc_avg'] = temp_sc_series
df['temp_sa'] = df['temp_avg'] - df['temp_sc_avg']

plot_periodogram(df.loc[df.index >= '1964-01-02', 'temp_sa'].dropna())

print('p-value of ADF test (temp_sa):')
print(adfuller(df.loc[df.index >= '1964-01-02', 'temp_sa'].dropna())[1])
print('p-value of KPSS test (temp_sa):')
print(kpss(df.loc[df.index >= '1964-01-02', 'temp_sa'].dropna())[1])

# Difference if needed (to address potential stochastic trend)
df['temp_sa_d'] = df['temp_sa'].diff()
print('p-value of ADF test (temp_sa_d):')
print(adfuller(df.loc[df.index >= '1964-01-02', 'temp_sa_d'].dropna())[1])
print('p-value of KPSS test (temp_sa_d):')
print(kpss(df.loc[df.index >= '1964-01-02', 'temp_sa_d'].dropna())[1])













